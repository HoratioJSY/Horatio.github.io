<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <!--Description-->
    
        <meta name="description" content="测试代码，无高亮，，哈哈哈哈%……¥¥%@#……）
BASH 命令：
$ hexo new &#34;My New Post&#34;
444444444$ hexo server
python 代码（有木有高亮T_T）：
def inference(input_tensor, train, regularizer">
    

    <!--Author-->
    
        <meta name="author" content="Horatio">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Test Highlight"/>
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Hello Autumn"/>

    <!--Page Cover-->
    
        <meta property="og:image" content=""/>
    

    <!-- Title -->
    
    <title>Test Highlight - Hello Autumn</title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/sass/main.css">

    <!--[if lt IE 8]>
        <script src="/js/ie/html5shiv.js"></script>
    <![endif]-->

    <!--[if lt IE 8]>
        <link rel="stylesheet" href="/sass/ie8.css">
    <![endif]-->

    <!--[if lt IE 9]>
        <link rel="stylesheet" href="/sass/ie9.css">
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    


<link rel="shortcut icon" href="http://p598yuf6e.bkt.clouddn.com/favicon.ico" type="image/x-icon"/>
<link rel="icon" href="http://p598yuf6e.bkt.clouddn.com/favicon.ico" type="image/x-icon"/>
<link rel="apple-touch-icon" href="http://p598yuf6e.bkt.clouddn.com/favicon.ico"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="wrapper">

        <!-- Menu -->
        <!-- Header -->
<header id="header">
    <div class="inner">

        <!-- Logo -->
        <a href="/" class="logo">
            <span class="symbol"><img src="http://p598yuf6e.bkt.clouddn.com/BrainIcon.png" alt="" /></span><span class="title">Hello Autumn</span>
        </a>

        <!-- Nav -->
        <nav>
            <ul>
                <li><a href="#menu">Menu</a></li>
            </ul>
        </nav>

    </div>
</header>

<!-- Menu -->
<nav id="menu">
    <h2>Menu</h2>
    <ul>
        
            <li>
                <a href="/">Me</a>
            </li>
        
            <li>
                <a href="/">Photographer</a>
            </li>
        
            <li>
                <a href="/">University life</a>
            </li>
        
            <li>
                <a href="/archives">Archives</a>
            </li>
        
            <li>
                <a href="/">About</a>
            </li>
        
    </ul>
</nav>


        <div id="main">
            <div class="inner">

                <!-- Main Content -->
                

    <h1>Test Highlight</h1>



<!-- Gallery -->


<!-- Content -->
<p>测试代码，无高亮，，哈哈哈哈%……¥¥%@#……<em>）</em></p>
<p>BASH 命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>444444444<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure></p>
<p>python 代码（有木有高亮T_T）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor, train, regularizer)</span>:</span></span><br><span class="line">    <span class="comment"># 声明第一层卷积层的变量并实现前向传播过程。通过使用不同命名空间来隔离不同层的变量，让每一层中的变量命名只需要考虑在当前层的作用，</span></span><br><span class="line">    <span class="comment"># 不需担心重命名的问题。第一层输出为28×28×32的张量</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer1-conv1'</span>):</span><br><span class="line">        conv1_weights = tf.get_variable(<span class="string">'weight'</span>, [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],</span><br><span class="line">                                        initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        conv1_biases = tf.get_variable(<span class="string">'bias'</span>, [CONV1_DEEP], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用边长为5，深度为32的卷积核，卷积核的移动步幅为1，且使用0填充</span></span><br><span class="line">        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实现第二层池化层的前向传播过程。该最大池化层卷积核边长为2，使用0填充，移动步幅为2.</span></span><br><span class="line">    <span class="comment"># 该层的输入为28×28×32的张量，输出为14×14×32的张量</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer2-pool1'</span>):</span><br><span class="line">        pool1 = tf.nn.max_pool(relu1, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 声明第三层卷积层的变量并实现前向传播过程，该卷积层的输入为14×14×32的张量，输出为14×14×64的矩阵</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer3-conv2'</span>):</span><br><span class="line">        conv2_weights = tf.get_variable(<span class="string">'weight'</span>, [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],</span><br><span class="line">                                        initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        conv2_biases = tf.get_variable(<span class="string">'bias'</span>, [CONV2_DEEP], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用尺寸为5×5，深度为64的卷积核，卷积核的移动步幅为1，且使用0填充</span></span><br><span class="line">        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实现第四层池化层的前向传播过程，输入为14×14×64，输出为7×7×64的张量</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer4-pool2'</span>):</span><br><span class="line">        pool2 = tf.nn.max_pool(relu2, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将第四层池化层的输出转化为第五层全连接层的输入格式。第四层为7×7×64的张量，第五层输入为向量，所以需要将该张量拉成一个向量</span></span><br><span class="line">    <span class="comment"># pool2.get_shape函数取第四层输出张量的维度，每层的输入输出都为一个BATCH的张量，所以这里得到的维度也包含一个BATCH中数据的数量。</span></span><br><span class="line">    pool_shape = pool2.get_shape().as_list()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算将张量拉直成向量后的长度，该长度等于张量维度累乘。注意这里的pool_shape[0]为一个batch中数据的个数</span></span><br><span class="line">    nodes = pool_shape[<span class="number">1</span>] * pool_shape[<span class="number">2</span>] * pool_shape[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过tf.reshape函数将第四层的输出变成一个batch的向量</span></span><br><span class="line">    reshaped = tf.reshape(pool2, [pool_shape[<span class="number">0</span>], nodes])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 声明第五层全连接层的变量并实现前向传播过程。输入长度为3136的向量，输出长度为512的向量。该层引入了dropout的概念，</span></span><br><span class="line">    <span class="comment"># dropout在训练时随机将部分结点的输出改为0.dropout一般只在全连接层而不是卷积层或池化层使用。</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer5-fcl'</span>):</span><br><span class="line">        fc1_weights = tf.get_variable(<span class="string">'weight'</span>, [nodes, FC_SIZE],</span><br><span class="line">                                      initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 只有全连接层权重需要加入正则化</span></span><br><span class="line">        <span class="keyword">if</span> regularizer != <span class="keyword">None</span>:</span><br><span class="line">            tf.add_to_collection(<span class="string">'losses'</span>, regularizer(fc1_weights))</span><br><span class="line">        fc1_biases = tf.get_variable(<span class="string">'bias'</span>, [FC_SIZE], initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line">        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)</span><br><span class="line">        <span class="keyword">if</span> train: fc1 = tf.nn.dropout(fc1, <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 声明第六层全连接层变量并实现前向传播，输入长度为512的向量，输出长度为10的向量。输出通过softmax之后可得到最后的分类结果。</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer6-fc2'</span>):</span><br><span class="line">        fc2_weights = tf.get_variable(<span class="string">'weight'</span>, [FC_SIZE, NUM_LABELS],</span><br><span class="line">                                      initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        <span class="keyword">if</span> regularizer != <span class="keyword">None</span>:</span><br><span class="line">            tf.add_to_collection(<span class="string">'losses'</span>, regularizer(fc1_weights))</span><br><span class="line"></span><br><span class="line">        fc2_biases = tf.get_variable(<span class="string">'bias'</span>, [NUM_LABELS], initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line">        logit = tf.matmul(fc1, fc2_weights) + fc2_biases</span><br><span class="line">    <span class="keyword">return</span> logit</span><br></pre></td></tr></table></figure>
<p>另一种：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer1-conv1'</span>):</span><br><span class="line">        conv1_weights = tf.get_variable(<span class="string">'weight'</span>, [CONV1_SIZE, CONV1_SIZE, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                        initializer=tf.constant_initializer(W[<span class="number">0</span>:<span class="number">2</span>, <span class="number">0</span>:<span class="number">2</span>]))</span><br><span class="line">        conv1_biases = tf.get_variable(<span class="string">'bias'</span>, [<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">        relu1 = tf.nn.sigmoid(tf.nn.bias_add(conv1, conv1_biases))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer2-pool1'</span>):</span><br><span class="line">        pool1 = tf.nn.max_pool(relu1, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer3-conv2'</span>):</span><br><span class="line">        conv2_weights = tf.get_variable(<span class="string">'weight'</span>, [CONV2_SIZE, CONV2_SIZE, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                        initializer=tf.constant_initializer(W[<span class="number">0</span>:<span class="number">3</span>, <span class="number">0</span>:<span class="number">3</span>]))</span><br><span class="line">        conv2_biases = tf.get_variable(<span class="string">'bias'</span>, [<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">        relu2 = tf.nn.sigmoid(tf.nn.bias_add(conv2, conv2_biases))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer4-pool2'</span>):</span><br><span class="line">        pool2 = tf.nn.max_pool(relu2, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], activation_fn=tf.nn.sigmoid, stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer5-Inception_v3-Module'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_0'</span>):</span><br><span class="line">                branch_0 = slim.conv2d(pool2, <span class="number">1</span>, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                       weights_initializer=tf.constant_initializer(W[<span class="number">3</span>:<span class="number">4</span>, <span class="number">3</span>:<span class="number">4</span>]), scope=<span class="string">'Ince_0'</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_1'</span>):</span><br><span class="line">                branch_1 = slim.conv2d(pool2, <span class="number">1</span>, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                       weights_initializer=tf.constant_initializer(W[<span class="number">4</span>:<span class="number">5</span>, <span class="number">4</span>:<span class="number">5</span>]), scope=<span class="string">'Ince_1_1'</span>)</span><br><span class="line">                branch_1 = tf.concat([slim.conv2d(branch_1, <span class="number">32</span>, [<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">                                                     weights_initializer=tf.constant_initializer(W[<span class="number">3</span>:<span class="number">4</span>, <span class="number">1</span>:<span class="number">4</span>]), scope=<span class="string">'Ince_1_2a'</span>),</span><br><span class="line">                                         slim.conv2d(branch_1, <span class="number">32</span>, [<span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                                                     weights_initializer=tf.constant_initializer(W[<span class="number">1</span>:<span class="number">4</span>, <span class="number">3</span>:<span class="number">4</span>]), scope=<span class="string">'Ince_1_2b'</span>)], <span class="number">3</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_2'</span>):</span><br><span class="line">                branch_2 = slim.conv2d(pool2, <span class="number">1</span>, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                       weights_initializer=tf.constant_initializer(W[<span class="number">4</span>:<span class="number">5</span>, <span class="number">4</span>:<span class="number">5</span>]), scope=<span class="string">'Ince_2_1'</span>)</span><br><span class="line">                branch_2 = slim.conv2d(branch_2, <span class="number">1</span>, [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                       weights_initializer=tf.constant_initializer(W[<span class="number">0</span>:<span class="number">3</span>, <span class="number">0</span>:<span class="number">3</span>]), scope=<span class="string">'Ince_2_2'</span>)</span><br><span class="line">                branch_2 = tf.concat([slim.conv2d(branch_2, <span class="number">1</span>, [<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">                                                     weights_initializer=tf.constant_initializer(W[<span class="number">0</span>:<span class="number">1</span>, <span class="number">0</span>:<span class="number">3</span>]), scope=<span class="string">'Ince_2_3a'</span>),</span><br><span class="line">                                         slim.conv2d(branch_2, <span class="number">1</span>, [<span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                                                     weights_initializer=tf.constant_initializer(W[<span class="number">0</span>:<span class="number">3</span>, <span class="number">0</span>:<span class="number">1</span>]), scope=<span class="string">'Ince_2_3b'</span>)], <span class="number">3</span>)</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'Branch_3'</span>):</span><br><span class="line">                <span class="comment"># branch_3 = slim.avg_pool2d(pool2, [3, 3],scope='Ince_3_1')</span></span><br><span class="line">                branch_3 = slim.conv2d(pool2, <span class="number">1</span>, [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                                       weights_initializer=tf.constant_initializer(W[<span class="number">4</span>:<span class="number">5</span>, <span class="number">4</span>:<span class="number">5</span>]), scope=<span class="string">'Ince_3_2'</span>)</span><br><span class="line">            inception = tf.concat([branch_0, branch_1, branch_2, branch_3], <span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">    inception_shape = inception.get_shape().as_list()</span><br><span class="line">    nodes = inception_shape[<span class="number">1</span>] * inception_shape[<span class="number">2</span>] * inception_shape[<span class="number">3</span>]</span><br><span class="line">    reshaped = tf.reshape(inception, [<span class="number">1</span>, nodes])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer6-fc1'</span>):</span><br><span class="line">        fc1_weights = tf.get_variable(<span class="string">'weight'</span>, [nodes, FC_SIZE],</span><br><span class="line">                                      initializer=tf.truncated_normal_initializer(stddev=<span class="number">3</span>, seed=<span class="number">3</span>), trainable=<span class="keyword">False</span>)</span><br><span class="line">        fc1_biases = tf.get_variable(<span class="string">'bias'</span>, [FC_SIZE], initializer=tf.constant_initializer(<span class="number">-10.0</span>))</span><br><span class="line">        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer7-fc2'</span>):</span><br><span class="line">        fc2_weights = tf.get_variable(<span class="string">'weight'</span>, [FC_SIZE, OUTPUT_NODE],</span><br><span class="line">                                      initializer=tf.constant_initializer(<span class="number">0.0001</span>))</span><br><span class="line">        fc2_biases = tf.get_variable(<span class="string">'bias'</span>, [OUTPUT_NODE], initializer=tf.constant_initializer(<span class="number">-11.0</span>))</span><br><span class="line">        secret = tf.matmul(fc1, fc2_weights) + fc2_biases</span><br><span class="line">    <span class="keyword">return</span> secret</span><br></pre></td></tr></table></figure>
<p>天呐，要是木有高亮咋办～～</p>


<!-- Tags -->



<div class="tags">
    
</div>



<!-- Comments -->
<div>
    
    <hr />
    <h3>Kommentare:</h3>
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>



</div>



            </div>
        </div>

        <!-- Footer -->
<footer id="footer">
    <div class="inner">
        <section>
            <h2>About</h2>
            <div>
                If you are also interesting in Machine Learning, contact me with email：horatio.j.s.y@gmail.com.
            </div>
        </section>
        <section>
            <h2>Follow</h2>
            <ul class="icons">
                
                    <li><a href="https://twitter.com/Horatio_JSY" class="icon style2 fa-twitter" target="_blank" ><span class="label">Twitter</span></a></li>
                
                
                    <li><a href="https://www.facebook.com/horatio.jsy.3" class="icon style2 fa-facebook" target="_blank" ><span class="label">Facebook</span></a></li>
                
                
                
                
                    <li><a href="https://github.com/HoratioJSY" class="icon style2 fa-github" target="_blank" ><span class="label">GitHub</span></a></li>
                
                
                
                
                    <li><a href="https://500px.com/siyuan" class="icon style2 fa-500px" target="_blank" ><span class="label">500px</span></a></li>
                
                
                    <li><a href="horatio.j.s.y@gmail.com" class="icon style2 fa-envelope-o" target="_blank" ><span class="label">Email</span></a></li>
                
                
                    <li><a href="\#" class="icon style2 fa-rss" target="_blank" ><span class="label">RSS</span></a></li>
                
            </ul>
        </section>
        <ul class="copyright">
            <li>&copy; Siyuan All rights reserved</li>
            <li>Design: Horatio</li>
        </ul>
    </div>
</footer>
    </div>

    <!-- After footer scripts -->
    <script type="text/javascript"
color="139,129,80" opacity='0.4' zIndex="-2" count="60" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- skel -->
<script src="/js/skel.min.js"></script>

<!-- Custom Code -->
<script src="/js/util.js"></script>

<!--[if lte IE 8]>
<script src="/js/ie/respond.min.js"></script>
<![endif]-->

<!-- Custom Code -->
<script src="/js/main.js"></script>

<!-- Gallery -->
<script src="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->

<script type="text/javascript">
    var disqus_shortname = 'Autum';

    (function(){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</body>

</html>